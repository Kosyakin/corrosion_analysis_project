import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import BayesianRidge
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("‚ö†Ô∏è Optuna –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –î–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install optuna")

try:
    from skopt import gp_minimize
    from skopt.space import Real, Integer
    from skopt.utils import use_named_args
    SKOPT_AVAILABLE = True
except ImportError:
    SKOPT_AVAILABLE = False
    # –ù–µ –≤—ã–≤–æ–¥–∏–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∑–¥–µ—Å—å, —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–æ–ø—ã—Ç–∫–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å


class BayesianCorrosionAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –±–∞–π–µ—Å–æ–≤—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–æ–∑–∏–∏ —Å –æ—Ü–µ–Ω–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    """
    
    def __init__(self, data: pd.DataFrame):
        """
        Parameters:
        -----------
        data : pd.DataFrame
            DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        self.data = data.copy()
        self.results = {}
        self.target = None
    
    def set_target(self, target_column: str):
        """
        –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        
        Parameters:
        -----------
        target_column : str
            –ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
        """
        if target_column not in self.data.columns:
            raise ValueError(f"–ö–æ–ª–æ–Ω–∫–∞ '{target_column}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –¥–∞–Ω–Ω—ã—Ö")
        self.target = target_column
        return self
    
    def train_bayesian_ridge(self, feature_columns: list,
                            test_size: float = 0.2,
                            random_state: int = 42,
                            n_iter: int = 300,
                            compute_score: bool = True) -> dict:
        """
        –û–±—É—á–µ–Ω–∏–µ Bayesian Ridge —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ —Å –æ—Ü–µ–Ω–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        
        Parameters:
        -----------
        feature_columns : list
            –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        test_size : float
            –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        random_state : int
            Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        n_iter : int
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        compute_score : bool
            –í—ã—á–∏—Å–ª—è—Ç—å –ª–∏ score –º–æ–¥–µ–ª–∏
            
        Returns:
        --------
        dict
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
            - metrics: –º–µ—Ç—Ä–∏–∫–∏ (r2, mae, rmse)
            - uncertainty: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            - y_pred: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ
            - y_test: –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–µ
        """
        if not self.target:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é set_target()")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        valid_features = [f for f in feature_columns if f in self.data.columns]
        if len(valid_features) != len(feature_columns):
            missing = set(feature_columns) - set(valid_features)
            print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing}")
        
        model_data = self.data[valid_features + [self.target]].dropna()
        
        if len(model_data) < 50:
            raise ValueError(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(model_data)} —Å—Ç—Ä–æ–∫")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X = model_data[valid_features]
        y = model_data[self.target]
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        # –û–±—É—á–µ–Ω–∏–µ Bayesian Ridge
        model = Pipeline([
            ('scaler', StandardScaler()),
            ('model', BayesianRidge(
                n_iter=n_iter,
                compute_score=compute_score,
                alpha_1=1e-6,
                alpha_2=1e-6,
                lambda_1=1e-6,
                lambda_2=1e-6
            ))
        ])
        
        model.fit(X_train, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –æ—Ü–µ–Ω–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        y_pred_mean = model.predict(X_test)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
        # BayesianRidge –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–∞–ø—Ä—è–º—É—é std –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π,
        # –Ω–æ –º–æ–∂–µ–º –æ—Ü–µ–Ω–∏—Ç—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –¥–∏—Å–ø–µ—Ä—Å–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ pipeline
            br_model = model.named_steps['model']
            # –û—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–∏—Å–ø–µ—Ä—Å–∏—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
            X_test_scaled = model.named_steps['scaler'].transform(X_test)
            # –û—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –¥–∏—Å–ø–µ—Ä—Å–∏—é –≤–µ—Å–æ–≤
            # –≠—Ç–æ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if hasattr(br_model, 'sigma_') and br_model.sigma_ is not None:
                # –î–∏—Å–ø–µ—Ä—Å–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π = X @ sigma_ @ X.T
                pred_var = np.diag(X_test_scaled @ br_model.sigma_ @ X_test_scaled.T)
                y_pred_std = np.sqrt(pred_var + 1.0 / br_model.alpha_)
            else:
                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ alpha (—Ç–æ—á–Ω–æ—Å—Ç—å —à—É–º–∞)
                y_pred_std = np.sqrt(1.0 / br_model.alpha_) * np.ones(len(X_test))
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ü–µ–Ω–∏—Ç—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å: {e}")
            y_pred_std = None
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'r2': r2_score(y_test, y_pred_mean),
            'mae': mean_absolute_error(y_test, y_pred_mean),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred_mean))
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        result = {
            'model': model,
            'metrics': metrics,
            'uncertainty': y_pred_std,
            'y_pred': y_pred_mean,
            'y_test': y_test.values,
            'features': valid_features,
            'n_samples': len(model_data),
            'alpha': br_model.alpha_ if hasattr(br_model, 'alpha_') else None,
            'lambda': br_model.lambda_ if hasattr(br_model, 'lambda_') else None
        }
        
        self.results['bayesian_ridge'] = result
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_bayesian_ridge_results(result)
        
        return result
    
    def optimize_random_forest_bayesian(self, feature_columns: list,
                                       n_trials: int = 50,
                                       test_size: float = 0.2,
                                       random_state: int = 42,
                                       cv_folds: int = 5,
                                       use_optuna: bool = True) -> dict:
        """
        –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Random Forest
        
        Parameters:
        -----------
        feature_columns : list
            –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        n_trials : int
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        test_size : float
            –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        random_state : int
            Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        cv_folds : int
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–¥–æ–≤ –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        use_optuna : bool
            –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Optuna (True) –∏–ª–∏ scikit-optimize (False)
            
        Returns:
        --------
        dict
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - best_params: –ª—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            - best_score: –ª—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            - best_model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            - metrics: –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
            - study: –æ–±—ä–µ–∫—Ç study (–¥–ª—è Optuna)
        """
        if not self.target:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å –ø–æ–º–æ—â—å—é set_target()")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫
        if use_optuna and not OPTUNA_AVAILABLE:
            print("‚ö†Ô∏è Optuna –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º scikit-optimize")
            use_optuna = False
        
        if not use_optuna and not SKOPT_AVAILABLE:
            print("‚ö†Ô∏è scikit-optimize –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é Optuna –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É...")
            if OPTUNA_AVAILABLE:
                use_optuna = True
            else:
                raise ValueError("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –±–∏–±–ª–∏–æ—Ç–µ–∫—É: pip install optuna –∏–ª–∏ pip install scikit-optimize")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        valid_features = [f for f in feature_columns if f in self.data.columns]
        if len(valid_features) != len(feature_columns):
            missing = set(feature_columns) - set(valid_features)
            print(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–æ–ª–æ–Ω–∫–∏: {missing}")
        
        model_data = self.data[valid_features + [self.target]].dropna()
        
        if len(model_data) < 50:
            raise ValueError(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(model_data)} —Å—Ç—Ä–æ–∫")
        
        X = model_data[valid_features]
        y = model_data[self.target]
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        if use_optuna:
            return self._optimize_with_optuna(
                X_train, y_train, X_test, y_test, 
                valid_features, n_trials, cv_folds, random_state
            )
        else:
            return self._optimize_with_skopt(
                X_train, y_train, X_test, y_test,
                valid_features, n_trials, cv_folds, random_state
            )
    
    def _optimize_with_optuna(self, X_train, y_train, X_test, y_test,
                              features, n_trials, cv_folds, random_state):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é Optuna"""
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å Optuna ({n_trials} trials)...")
        
        def objective(trial):
            params = {
                'n_estimators': trial.suggest_int('n_estimators', 50, 500),
                'max_depth': trial.suggest_int('max_depth', 5, 30),
                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),
                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),
                'random_state': random_state,
                'n_jobs': -1
            }
            
            model = RandomForestRegressor(**params)
            scores = cross_val_score(
                model, X_train, y_train, 
                cv=cv_folds, 
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            return -scores.mean()
        
        study = optuna.create_study(direction='minimize')
        try:
            # Optuna 3.0+ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç show_progress_bar, —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ - –Ω–µ—Ç
            study.optimize(objective, n_trials=n_trials)
        except TypeError:
            # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π optuna
            study.optimize(objective, n_trials=n_trials)
        
        # –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_params = study.best_params.copy()
        best_params['random_state'] = random_state
        best_params['n_jobs'] = -1
        
        best_model = RandomForestRegressor(**best_params)
        best_model.fit(X_train, y_train)
        y_pred = best_model.predict(X_test)
        
        metrics = {
            'r2': r2_score(y_test, y_pred),
            'mae': mean_absolute_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred))
        }
        
        result = {
            'best_params': best_params,
            'best_score': study.best_value,
            'best_model': best_model,
            'metrics': metrics,
            'study': study,
            'features': features,
            'y_pred': y_pred,
            'y_test': y_test.values
        }
        
        self.results['optimized_rf'] = result
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_optimization_results(result, 'Optuna')
        
        return result
    
    def _optimize_with_skopt(self, X_train, y_train, X_test, y_test,
                             features, n_trials, cv_folds, random_state):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é scikit-optimize"""
        print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –±–∞–π–µ—Å–æ–≤—Å–∫—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Å scikit-optimize ({n_trials} trials)...")
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
        space = [
            Integer(50, 500, name='n_estimators'),
            Integer(5, 30, name='max_depth'),
            Integer(2, 20, name='min_samples_split'),
            Integer(1, 10, name='min_samples_leaf'),
        ]
        
        @use_named_args(space=space)
        def objective(**params):
            params['random_state'] = random_state
            params['n_jobs'] = -1
            model = RandomForestRegressor(**params)
            scores = cross_val_score(
                model, X_train, y_train,
                cv=cv_folds,
                scoring='neg_mean_squared_error',
                n_jobs=-1
            )
            return -scores.mean()
        
        result_gp = gp_minimize(
            objective, space, n_calls=n_trials,
            random_state=random_state,
            n_jobs=1,
            verbose=True
        )
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        best_params = {
            'n_estimators': result_gp.x[0],
            'max_depth': result_gp.x[1],
            'min_samples_split': result_gp.x[2],
            'min_samples_leaf': result_gp.x[3],
            'random_state': random_state,
            'n_jobs': -1
        }
        
        # –û–±—É—á–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        best_model = RandomForestRegressor(**best_params)
        best_model.fit(X_train, y_train)
        y_pred = best_model.predict(X_test)
        
        metrics = {
            'r2': r2_score(y_test, y_pred),
            'mae': mean_absolute_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred))
        }
        
        result = {
            'best_params': best_params,
            'best_score': result_gp.fun,
            'best_model': best_model,
            'metrics': metrics,
            'optimization_result': result_gp,
            'features': features,
            'y_pred': y_pred,
            'y_test': y_test.values
        }
        
        self.results['optimized_rf'] = result
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._print_optimization_results(result, 'scikit-optimize')
        
        return result
    
    def compare_bayesian_methods(self, feature_columns: list,
                                test_size: float = 0.2,
                                random_state: int = 42,
                                n_trials: int = 30) -> pd.DataFrame:
        """
        –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Bayesian Ridge –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ Random Forest
        
        Parameters:
        -----------
        feature_columns : list
            –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        test_size : float
            –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        random_state : int
            Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        n_trials : int
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ RF
            
        Returns:
        --------
        pd.DataFrame
            DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        print("=" * 70)
        print("üî¨ –°–†–ê–í–ù–ï–ù–ò–ï –ë–ê–ô–ï–°–û–í–°–ö–ò–• –ú–ï–¢–û–î–û–í")
        print("=" * 70)
        
        results = []
        
        # 1. Bayesian Ridge
        print("\n1Ô∏è‚É£ –û–±—É—á–µ–Ω–∏–µ Bayesian Ridge...")
        try:
            br_result = self.train_bayesian_ridge(
                feature_columns=feature_columns,
                test_size=test_size,
                random_state=random_state
            )
            results.append({
                'method': 'Bayesian Ridge',
                'r2': br_result['metrics']['r2'],
                'mae': br_result['metrics']['mae'],
                'rmse': br_result['metrics']['rmse'],
                'has_uncertainty': br_result['uncertainty'] is not None
            })
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ Bayesian Ridge: {e}")
        
        # 2. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Random Forest
        print("\n2Ô∏è‚É£ –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è Random Forest...")
        try:
            rf_result = self.optimize_random_forest_bayesian(
                feature_columns=feature_columns,
                n_trials=n_trials,
                test_size=test_size,
                random_state=random_state
            )
            results.append({
                'method': 'RF (Bayesian Optimized)',
                'r2': rf_result['metrics']['r2'],
                'mae': rf_result['metrics']['mae'],
                'rmse': rf_result['metrics']['rmse'],
                'best_params': str(rf_result['best_params']),
                'has_uncertainty': False
            })
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ RF: {e}")
        
        # 3. –ë–∞–∑–æ–≤—ã–π Random Forest –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        print("\n3Ô∏è‚É£ –ë–∞–∑–æ–≤—ã–π Random Forest (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)...")
        try:
            valid_features = [f for f in feature_columns if f in self.data.columns]
            model_data = self.data[valid_features + [self.target]].dropna()
            X = model_data[valid_features]
            y = model_data[self.target]
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state
            )
            
            baseline_rf = RandomForestRegressor(
                n_estimators=100,
                random_state=random_state,
                n_jobs=-1
            )
            baseline_rf.fit(X_train, y_train)
            y_pred = baseline_rf.predict(X_test)
            
            results.append({
                'method': 'RF (Baseline)',
                'r2': r2_score(y_test, y_pred),
                'mae': mean_absolute_error(y_test, y_pred),
                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                'has_uncertainty': False
            })
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞–∑–æ–≤–æ–º RF: {e}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ DataFrame –∏ –≤—ã–≤–æ–¥
        results_df = pd.DataFrame(results)
        
        print("\n" + "=" * 70)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–†–ê–í–ù–ï–ù–ò–Ø")
        print("=" * 70)
        print(results_df.to_string(index=False))
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        self._plot_bayesian_comparison(results_df)
        
        self.results['comparison'] = results_df
        return results_df
    
    def plot_uncertainty(self, result_key: str = 'bayesian_ridge', 
                        n_samples: int = 100):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π Bayesian Ridge
        
        Parameters:
        -----------
        result_key : str
            –ö–ª—é—á —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ self.results
        n_samples : int
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤—ã–±–æ—Ä–æ–∫ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        """
        if result_key not in self.results:
            print(f"‚ùå –†–µ–∑—É–ª—å—Ç–∞—Ç '{result_key}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return
        
        result = self.results[result_key]
        
        if result.get('uncertainty') is None:
            print("‚ö†Ô∏è –û—Ü–µ–Ω–∫–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return
        
        y_test = result['y_test']
        y_pred = result['y_pred']
        y_std = result['uncertainty']
        
        # –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        
        # 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è vs –∏—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        axes[0].scatter(y_test, y_pred, alpha=0.5, s=20)
        axes[0].plot([y_test.min(), y_test.max()], 
                    [y_test.min(), y_test.max()], 
                    'r--', lw=2, label='–ò–¥–µ–∞–ª—å–Ω–∞—è –ª–∏–Ω–∏—è')
        axes[0].set_xlabel('–ò—Å—Ç–∏–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)
        axes[0].set_ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)
        axes[0].set_title('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è Bayesian Ridge', fontweight='bold')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. –û—Å—Ç–∞—Ç–∫–∏ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        residuals = y_test - y_pred
        axes[1].errorbar(y_pred, residuals, yerr=y_std, 
                        fmt='o', alpha=0.5, markersize=4, capsize=2)
        axes[1].axhline(y=0, color='r', linestyle='--', lw=2)
        axes[1].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è', fontsize=12)
        axes[1].set_ylabel('–û—Å—Ç–∞—Ç–∫–∏', fontsize=12)
        axes[1].set_title('–û—Å—Ç–∞—Ç–∫–∏ —Å –æ—Ü–µ–Ω–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏', fontweight='bold')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def _print_bayesian_ridge_results(self, result):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ Bayesian Ridge"""
        print("=" * 70)
        print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ BAYESIAN RIDGE")
        print("=" * 70)
        print(f"   R¬≤ = {result['metrics']['r2']:7.4f}")
        print(f"   MAE = {result['metrics']['mae']:7.4f}")
        print(f"   RMSE = {result['metrics']['rmse']:7.4f}")
        if result.get('alpha'):
            print(f"   Alpha (—Ç–æ—á–Ω–æ—Å—Ç—å —à—É–º–∞) = {result['alpha']:.6f}")
        if result.get('lambda'):
            print(f"   Lambda (—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è) = {result['lambda']:.6f}")
        if result['uncertainty'] is not None:
            print(f"   –°—Ä–µ–¥–Ω—è—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å = {result['uncertainty'].mean():.6f}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(result['features'])}")
        print(f"   –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {result['n_samples']}")
    
    def _print_optimization_results(self, result, method_name: str):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        print("=" * 70)
        print(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò ({method_name})")
        print("=" * 70)
        print(f"   –õ—É—á—à–∏–π score (CV): {result['best_score']:.6f}")
        print(f"   R¬≤ (test) = {result['metrics']['r2']:7.4f}")
        print(f"   MAE (test) = {result['metrics']['mae']:7.4f}")
        print(f"   RMSE (test) = {result['metrics']['rmse']:7.4f}")
        print(f"\n   –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:")
        for key, value in result['best_params'].items():
            if key not in ['random_state', 'n_jobs']:
                print(f"     {key}: {value}")
    
    def _plot_bayesian_comparison(self, results_df):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –±–∞–π–µ—Å–æ–≤—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤"""
        if len(results_df) < 2:
            return
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        
        metrics = ['r2', 'mae', 'rmse']
        titles = ['R¬≤ (–±–æ–ª—å—à–µ ‚Üí –ª—É—á—à–µ)', 'MAE (–º–µ–Ω—å—à–µ ‚Üí –ª—É—á—à–µ)', 'RMSE (–º–µ–Ω—å—à–µ ‚Üí –ª—É—á—à–µ)']
        colors = ['lightblue', 'lightgreen', 'lightcoral']
        
        for idx, (metric, title, color) in enumerate(zip(metrics, titles, colors)):
            axes[idx].bar(results_df['method'], results_df[metric], color=color)
            axes[idx].set_title(title, fontweight='bold')
            axes[idx].tick_params(axis='x', rotation=45)
            axes[idx].grid(True, alpha=0.3)
            
            for i, v in enumerate(results_df[metric]):
                axes[idx].text(i, v, f'{v:.4f}', ha='center', va='bottom')
        
        plt.suptitle('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –±–∞–π–µ—Å–æ–≤—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.show()

